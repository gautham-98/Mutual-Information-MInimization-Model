@article{litjens_2017_a,
  author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen A.W.M. and van Ginneken, Bram and Sánchez, Clara I.},
  month = {12},
  pages = {60-88},
  title = {A Survey on Deep Learning in Medical Image Analysis},
  doi = {10.1016/j.media.2017.07.005},
  volume = {42},
  year = {2017},
  journal = {Medical Image Analysis}
}

@article{tsuneki_2022_deep,
  author = {Tsuneki, Masayuki},
  month = {03},
  title = {Deep learning models in medical image analysis},
  doi = {10.1016/j.job.2022.03.003},
  volume = {64},
  year = {2022},
  journal = {Journal of Oral Biosciences}
}

@article{sanchez_2022_causal,
  author = {Sanchez, Pedro and Voisey, Jeremy P. and Xia, Tian and Watson, Hannah I. and O’Neil, Alison Q. and Tsaftaris, Sotirios A.},
  month = {08},
  title = {Causal machine learning for healthcare and precision medicine},
  doi = {10.1098/rsos.220638},
  volume = {9},
  year = {2022},
  journal = {Royal Society Open Science}
}

@article{DBLP:journals/corr/abs-1807-04975,
  author       = {Sara Beery and
                  Grant Van Horn and
                  Pietro Perona},
  title        = {Recognition in Terra Incognita},
  journal      = {CoRR},
  volume       = {abs/1807.04975},
  year         = {2018},
  url          = {http://arxiv.org/abs/1807.04975},
  eprinttype    = {arXiv},
  eprint       = {1807.04975},
  timestamp    = {Mon, 13 Aug 2018 16:48:11 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1807-04975.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1911-08731,
  author       = {Shiori Sagawa and
                  Pang Wei Koh and
                  Tatsunori B. Hashimoto and
                  Percy Liang},
  title        = {Distributionally Robust Neural Networks for Group Shifts: On the Importance
                  of Regularization for Worst-Case Generalization},
  journal      = {CoRR},
  volume       = {abs/1911.08731},
  year         = {2019},
  url          = {http://arxiv.org/abs/1911.08731},
  eprinttype    = {arXiv},
  eprint       = {1911.08731},
  timestamp    = {Tue, 03 Dec 2019 14:15:54 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1911-08731.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{larrazabal_2020_gender,
  author = {Larrazabal, Agostina J. and Nieto, Nicolás and Peterson, Victoria and Milone, Diego H. and Ferrante, Enzo},
  month = {06},
  pages = {12592–12594},
  title = {Gender imbalance in medical imaging datasets produces biased classifiers for computer-aided diagnosis},
  doi = {10.1073/pnas.1919012117},
  url = {https://www.pnas.org/content/117/23/12592},
  urldate = {2021-01-31},
  volume = {117},
  year = {2020},
  journal = {Proceedings of the National Academy of Sciences}
}

@ARTICLE{788640,
  author={Vapnik, V.N.},
  journal={IEEE Transactions on Neural Networks}, 
  title={An overview of statistical learning theory}, 
  year={1999},
  volume={10},
  number={5},
  pages={988-999},
  keywords={Statistical learning;Machine learning;Pattern recognition;Loss measurement;Support vector machines;Algorithm design and analysis;Multidimensional systems;Risk management;Probability distribution},
  doi={10.1109/72.788640}}

@misc{lynch2023spawrious,
      title={Spawrious: A Benchmark for Fine Control of Spurious Correlation Biases}, 
      author={Aengus Lynch and Gbètondji J-S Dovonon and Jean Kaddour and Ricardo Silva},
      year={2023},
      eprint={2303.05470},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{dazrodrguez_2023_gender,
  author = {Díaz-Rodríguez, Natalia and Rūta Binkytė and Wafae Bakkali and Sannidhi Bookseller and Tubaro, Paola and Andrius Bacevičius and Sami Zhioua and Chatila, Raja},
  month = {05},
  pages = {103276-103276},
  publisher = {French National Centre for Scientific Research},
  title = {Gender and sex bias in COVID-19 epidemiological data through the lens of causality},
  doi = {10.1016/j.ipm.2023.103276},
  urldate = {2024-01-21},
  volume = {60},
  year = {2023},
  journal = {HAL (Le Centre pour la Communication Scientifique Directe)}
}

@article{DBLP:journals/corr/abs-2004-07780,
  author       = {Robert Geirhos and
                  J{\"{o}}rn{-}Henrik Jacobsen and
                  Claudio Michaelis and
                  Richard S. Zemel and
                  Wieland Brendel and
                  Matthias Bethge and
                  Felix A. Wichmann},
  title        = {Shortcut Learning in Deep Neural Networks},
  journal      = {CoRR},
  volume       = {abs/2004.07780},
  year         = {2020},
  url          = {https://arxiv.org/abs/2004.07780},
  eprinttype    = {arXiv},
  eprint       = {2004.07780},
  timestamp    = {Thu, 14 Oct 2021 09:17:14 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2004-07780.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{10162210,
  author={Fay, Louisa and Cobos, Erick and Yang, Bin and Gatidis, Sergios and Küstner, Thomas},
  journal={IEEE Access}, 
  title={Avoiding Shortcut-Learning by Mutual Information Minimization in Deep Learning-Based Image Processing}, 
  year={2023},
  volume={11},
  number={},
  pages={64070-64086},
  keywords={Correlation;Learning systems;Mutual information;Deep learning;Predictive models;Databases;Data models;Biomedical imaging;Causality;deep learning;medical image analysis;mutual information;shortcut learning},
  doi={10.1109/ACCESS.2023.3289397}}

@article{DBLP:journals/corr/abs-2106-00545,
  author       = {Victor Veitch and
                  Alexander D'Amour and
                  Steve Yadlowsky and
                  Jacob Eisenstein},
  title        = {Counterfactual Invariance to Spurious Correlations: Why and How to
                  Pass Stress Tests},
  journal      = {CoRR},
  volume       = {abs/2106.00545},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.00545},
  eprinttype    = {arXiv},
  eprint       = {2106.00545},
  timestamp    = {Wed, 09 Jun 2021 18:45:08 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-00545.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1801-04062,
  author       = {Ishmael Belghazi and
                  Sai Rajeswar and
                  Aristide Baratin and
                  R. Devon Hjelm and
                  Aaron C. Courville},
  title        = {{MINE:} Mutual Information Neural Estimation},
  journal      = {CoRR},
  volume       = {abs/1801.04062},
  year         = {2018},
  url          = {http://arxiv.org/abs/1801.04062},
  eprinttype    = {arXiv},
  eprint       = {1801.04062},
  timestamp    = {Mon, 13 Aug 2018 16:46:51 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1801-04062.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{peters_2017_elements,
  author = {Peters, Jonas and Dominik Janzing and Bernhard Schölkopf},
  publisher = {Mass},
  title = {Elements of causal inference : foundations and learning algorithms},
  year = {2017}
}

@article{ward_2012_spurious,
  author = {Ward, Andrew},
  month = {11},
  pages = {699-712},
  title = {“Spurious Correlations and Causal Inferences”},
  doi = {10.1007/s10670-012-9411-6},
  urldate = {2020-03-02},
  volume = {78},
  year = {2012},
  journal = {Erkenntnis}
}

@book{Pearl09,
  abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. Cited in more than 2,100 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research.},
  added-at = {2018-09-17T16:24:20.000+0200},
  address = {Cambridge, UK},
  author = {Pearl, Judea},
  biburl = {https://www.bibsonomy.org/bibtex/2378bd006b231f81cddd091429ceb0f80/flint63},
  doi = {10.1017/CBO9780511803161},
  edition = 2,
  file = {2009/Pearl09.pdf},
  interhash = {e7828b9bdefba2511eb63114c0c32b1b},
  intrahash = {378bd006b231f81cddd091429ceb0f80},
  isbn = {978-0-521-89560-6},
  isbn10 = {052189560X},
  keywords = {01901 103 ai algorithm book knowledge numerical processing theory},
  language = {american},
  owner = {flint},
  publisher = {Cambridge University Press},
  sortdate = {2009-12-01},
  subtitle = {Models, Reasoning, and Inference},
  timestamp = {2018-09-17T16:24:20.000+0200},
  title = {Causality},
  year = 2009
}

@article{DBLP:journals/corr/abs-2109-05642,
  author       = {Yifei Ming and
                  Hang Yin and
                  Yixuan Li},
  title        = {On the Impact of Spurious Correlation for Out-of-distribution Detection},
  journal      = {CoRR},
  volume       = {abs/2109.05642},
  year         = {2021},
  url          = {https://arxiv.org/abs/2109.05642},
  eprinttype    = {arXiv},
  eprint       = {2109.05642},
  timestamp    = {Fri, 18 Nov 2022 15:40:46 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2109-05642.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

﻿@Article{Li1990,
author={Li, Wentian},
title={Mutual information functions versus correlation functions},
journal={Journal of Statistical Physics},
year={1990},
month={Sep},
day={01},
volume={60},
number={5},
pages={823-837},
abstract={This paper studies one application of mutual information to symbolic sequences: the mutual information functionM(d). This function is compared with the more frequently used correlation function$\Gamma$(d). An exact relation betweenM(d) and$\Gamma$(d) is derived for binary sequences. For sequences with more than two symbols, no such general relation exists; in particular,$\Gamma$(d)=0 may or may not lead toM(d)=0. This linear, but not general, independence between symbols separated by a distance is studied for ternary sequences. Also included is the estimation of the finite-size effect on calculating mutual information. Finally, the concept of ``symbolic noise'' is discussed.},
issn={1572-9613},
doi={10.1007/BF01025996},
url={https://doi.org/10.1007/BF01025996}
}

@misc{ruihongqiuExplanationMutual,
	author = {},
	title = {{E}xplanation of {M}utual {I}nformation {N}eural {E}stimation --- ruihongqiu.github.io},
	howpublished = {\url{https://ruihongqiu.github.io/posts/2020/07/mine/}},
	year = {},
	note = {[Accessed 30-03-2024]},
}

@article{castro2019morphomnist,
    author = {Castro, Daniel C. and Tan, Jeremy and Kainz, Bernhard and Konukoglu, Ender and Glocker, Ben},
    title = {{Morpho-MNIST}: Quantitative Assessment and Diagnostics for Representation Learning},
    year = {2019},
    journal = {Journal of Machine Learning Research},
    volume = {20},
    number = {178},
    eprint = {arXiv:1809.10780}
}

@article{DBLP:journals/corr/abs-1901-07031,
  author       = {Jeremy Irvin and
                  Pranav Rajpurkar and
                  Michael Ko and
                  Yifan Yu and
                  Silviana Ciurea{-}Ilcus and
                  Christopher Chute and
                  Henrik Marklund and
                  Behzad Haghgoo and
                  Robyn L. Ball and
                  Katie S. Shpanskaya and
                  Jayne Seekins and
                  David A. Mong and
                  Safwan S. Halabi and
                  Jesse K. Sandberg and
                  Ricky Jones and
                  David B. Larson and
                  Curtis P. Langlotz and
                  Bhavik N. Patel and
                  Matthew P. Lungren and
                  Andrew Y. Ng},
  title        = {CheXpert: {A} Large Chest Radiograph Dataset with Uncertainty Labels
                  and Expert Comparison},
  journal      = {CoRR},
  volume       = {abs/1901.07031},
  year         = {2019},
  url          = {http://arxiv.org/abs/1901.07031},
  eprinttype    = {arXiv},
  eprint       = {1901.07031},
  timestamp    = {Fri, 23 Jun 2023 22:30:55 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1901-07031.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{10.1007/978-3-031-16431-6_62,
author="Ihler, Sontje
and Kuhnke, Felix
and Spindeldreier, Svenja",
editor="Wang, Linwei
and Dou, Qi
and Fletcher, P. Thomas
and Speidel, Stefanie
and Li, Shuo",
title="A Comprehensive Study of Modern Architectures and Regularization Approaches on CheXpert5000",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="654--663",
abstract="Computer aided diagnosis (CAD) has gained an increased amount of attention in the general research community over the last years as an example of a typical limited data application - with experiments on labeled 100k--200k datasets. Although these datasets are still small compared to natural image datasets like ImageNet1k, ImageNet21k and JFT, they are large for annotated medical datasets, where 1k--10k labeled samples are much more common. There is no baseline on which methods to build on in the low data regime. In this work we bridge this gap by providing an extensive study on medical image classification with limited annotations (5k). We present a study of modern architectures applied to a fixed low data regime of 5000 images on the CheXpert dataset. Conclusively we find that models pretrained on ImageNet21k achieve a higher AUC and larger models require less training steps. All models are quite well calibrated even though we only fine-tuned on 5000 training samples. All `modern' architectures have higher AUC than ResNet50. Regularization of Big Transfer Models with MixUp or Mean Teacher improves calibration, MixUp also improves accuracy. Vision Transformer achieve comparable or on par results to Big Transfer Models.",
isbn="978-3-031-16431-6"
}

